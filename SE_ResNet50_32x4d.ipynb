{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import skimage.io\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, RandomSampler, SequentialSampler\n",
    "import torchvision.models as models\n",
    "from torch.utils import model_zoo\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "\n",
    "from albumentations import Compose, Normalize, HorizontalFlip, VerticalFlip\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "height=256\n",
    "width=256\n",
    "lr=3e-4\n",
    "batch_size=2\n",
    "epochs=20\n",
    "num_workers=4\n",
    "seed=42\n",
    "target_size=6 \n",
    "target_col='isup_grade'\n",
    "n_fold=4\n",
    "warmup_factor=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    LOGGER.info(f'[{name}] start')\n",
    "    yield\n",
    "    LOGGER.info(f'[{name}] done in {time.time() - t0:.0f} s.')\n",
    "\n",
    "    \n",
    "def init_logger(log_file='train.log'):\n",
    "    from logging import getLogger, DEBUG, FileHandler,  Formatter,  StreamHandler\n",
    "    \n",
    "    log_format = '%(asctime)s %(levelname)s %(message)s'\n",
    "    \n",
    "    stream_handler = StreamHandler()\n",
    "    stream_handler.setLevel(DEBUG)\n",
    "    stream_handler.setFormatter(Formatter(log_format))\n",
    "    \n",
    "    file_handler = FileHandler(log_file)\n",
    "    file_handler.setFormatter(Formatter(log_format))\n",
    "    \n",
    "    logger = getLogger('PANDA')\n",
    "    logger.setLevel(DEBUG)\n",
    "    logger.addHandler(stream_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "LOG_FILE = 'train.log'\n",
    "LOGGER = init_logger(LOG_FILE)\n",
    "\n",
    "\n",
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_torch(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, df, labels, transform=None):\n",
    "        self.df = df\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_name = self.df['image_id'].values[index]\n",
    "        image = skimage.io.MultiImage(f'train_images/{file_name}.tiff')\n",
    "        image = cv2.resize(image[-1], (height, width))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "            \n",
    "        label = self.labels[index]\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(*, data):\n",
    "    \n",
    "    assert data in ('train', 'valid')\n",
    "    \n",
    "    if data == 'train':\n",
    "        return Compose([\n",
    "            HorizontalFlip(p=0.5),VerticalFlip(p=0.5),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                      std=[0.229, 0.224, 0.225]),ToTensorV2(),])\n",
    "    \n",
    "    elif data == 'valid':\n",
    "        return Compose([\n",
    "            Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]),ToTensorV2(),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = train.copy()\n",
    "train_labels = folds[target_col].values\n",
    "kf = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(folds.values, train_labels)):\n",
    "    folds.loc[val_index, 'fold'] = int(fold)\n",
    "folds['fold'] = folds['fold'].astype(int)\n",
    "folds.to_csv('folds.csv', index=None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We donwloaded the entire model but we prefer to write down the code to follow the steps and loading the model_state_dict later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEModule(nn.Module):\n",
    "\n",
    "    def __init__(self, channels, reduction):\n",
    "        super(SEModule, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1,\n",
    "                             padding=0)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1,\n",
    "                             padding=0)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        module_input = x\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return module_input * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEResNeXtBottleneck(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNeXt bottleneck with a Squeeze-and-Excitation module.\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n",
    "                 downsample=None, base_width=4):\n",
    "        super(SEResNeXtBottleneck, self).__init__()\n",
    "        width = math.floor(planes * (base_width / 64)) * groups\n",
    "        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False,\n",
    "                               stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(width)\n",
    "        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n",
    "                               padding=1, groups=groups, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(width)\n",
    "        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.se_module = SEModule(planes * 4, reduction=reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out = self.se_module(out) + residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SENet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, groups, reduction, dropout_p=0.2,\n",
    "                 inplanes=128, input_3x3=True, downsample_kernel_size=3,\n",
    "                 downsample_padding=1, num_classes=1000):\n",
    "        super(SENet, self).__init__()\n",
    "        self.inplanes = inplanes\n",
    "        if input_3x3:\n",
    "            layer0_modules = [\n",
    "                ('conv1', nn.Conv2d(3, 64, 3, stride=2, padding=1,\n",
    "                                    bias=False)),\n",
    "                ('bn1', nn.BatchNorm2d(64)),\n",
    "                ('relu1', nn.ReLU(inplace=True)),\n",
    "                ('conv2', nn.Conv2d(64, 64, 3, stride=1, padding=1,\n",
    "                                    bias=False)),\n",
    "                ('bn2', nn.BatchNorm2d(64)),\n",
    "                ('relu2', nn.ReLU(inplace=True)),\n",
    "                ('conv3', nn.Conv2d(64, inplanes, 3, stride=1, padding=1,\n",
    "                                    bias=False)),\n",
    "                ('bn3', nn.BatchNorm2d(inplanes)),\n",
    "                ('relu3', nn.ReLU(inplace=True)),\n",
    "            ]\n",
    "        else:\n",
    "            layer0_modules = [\n",
    "                ('conv1', nn.Conv2d(3, inplanes, kernel_size=7, stride=2,\n",
    "                                    padding=3, bias=False)),\n",
    "                ('bn1', nn.BatchNorm2d(inplanes)),\n",
    "                ('relu1', nn.ReLU(inplace=True)),\n",
    "            ] \n",
    "        layer0_modules.append(('pool', nn.MaxPool2d(3, stride=2,\n",
    "                                                    ceil_mode=True)))\n",
    "        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n",
    "        self.layer1 = self._make_layer(\n",
    "            block,\n",
    "            planes=64,\n",
    "            blocks=layers[0],\n",
    "            groups=groups,\n",
    "            reduction=reduction,\n",
    "            downsample_kernel_size=1,\n",
    "            downsample_padding=0\n",
    "        )\n",
    "        self.layer2 = self._make_layer(\n",
    "            block,\n",
    "            planes=128,\n",
    "            blocks=layers[1],\n",
    "            stride=2,\n",
    "            groups=groups,\n",
    "            reduction=reduction,\n",
    "            downsample_kernel_size=downsample_kernel_size,\n",
    "            downsample_padding=downsample_padding\n",
    "        )\n",
    "        self.layer3 = self._make_layer(\n",
    "            block,\n",
    "            planes=256,\n",
    "            blocks=layers[2],\n",
    "            stride=2,\n",
    "            groups=groups,\n",
    "            reduction=reduction,\n",
    "            downsample_kernel_size=downsample_kernel_size,\n",
    "            downsample_padding=downsample_padding\n",
    "        )\n",
    "        self.layer4 = self._make_layer(\n",
    "            block,\n",
    "            planes=512,\n",
    "            blocks=layers[3],\n",
    "            stride=2,\n",
    "            groups=groups,\n",
    "            reduction=reduction,\n",
    "            downsample_kernel_size=downsample_kernel_size,\n",
    "            downsample_padding=downsample_padding\n",
    "        )\n",
    "        self.avg_pool = nn.AvgPool2d(7, stride=1)\n",
    "        self.dropout = nn.Dropout(dropout_p) if dropout_p is not None else None\n",
    "        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, groups, reduction, stride=1,\n",
    "                    downsample_kernel_size=1, downsample_padding=0):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=downsample_kernel_size, stride=stride,\n",
    "                          padding=downsample_padding, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, groups, reduction, stride,\n",
    "                            downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups, reduction))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def features(self, x):\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "\n",
    "    def logits(self, x):\n",
    "        x = self.avg_pool(x)\n",
    "        if self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.last_linear(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.logits(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def se_resnext50_32x4d(num_classes=1000, pretrained='imagenet'):\n",
    "    model = SENet(SEResNeXtBottleneck, [3, 4, 6, 3], groups=32, reduction=16,\n",
    "                  dropout_p=None, inplanes=64, input_3x3=False,\n",
    "                  downsample_kernel_size=1, downsample_padding=0,\n",
    "                  num_classes=num_classes) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_setup():\n",
    "    \n",
    "    trn_idx = folds[folds['fold'] != fold].index\n",
    "    val_idx = folds[folds['fold'] == fold].index\n",
    "    global train_dataset, valid_dataset, train_loader, valid_loader\n",
    "    train_dataset = Dataset(folds.loc[trn_idx].reset_index(drop=True), \n",
    "                                folds.loc[trn_idx].reset_index(drop=True)[target_col], \n",
    "                                transform=get_transforms(data='train'))\n",
    "    valid_dataset = Dataset(folds.loc[val_idx].reset_index(drop=True), \n",
    "                                folds.loc[val_idx].reset_index(drop=True)[target_col], \n",
    "                                transform=get_transforms(data='valid'))\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=RandomSampler(train_dataset), num_workers=num_workers)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, sampler=SequentialSampler(valid_dataset), num_workers=num_workers)\n",
    "    \n",
    "    return \n",
    "\n",
    "def build_the_network():\n",
    "    \n",
    "    model_dict = {'se_resnext50_32x4d': 'se_resnext50_32x4d-a260b3a4.pth'}\n",
    "    \n",
    "    global model\n",
    "    model = se_resnext50_32x4d(pretrained=None)\n",
    "    model.load_state_dict(torch.load(model_dict['se_resnext50_32x4d']))\n",
    "    model.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "    model.last_linear = nn.Linear(model.last_linear.in_features, target_size)\n",
    "    model = model.to(device) \n",
    "    \n",
    "    return \n",
    "\n",
    "def optim():\n",
    "    \n",
    "    global optimizer, scheduler, criterion\n",
    "    optimizer = Adam(model.parameters(), lr=lr/warmup_factor, amsgrad=False)\n",
    "#scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbose=True, eps=1e-6)\n",
    "    scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs-1)\n",
    "    scheduler = GradualWarmupScheduler(optimizer, multiplier=warmup_factor, \n",
    "                                   total_epoch=1, \n",
    "                                   after_scheduler=scheduler_cosine)\n",
    "    criterion = nn.CrossEntropyLoss() \n",
    "    \n",
    "    return \n",
    "\n",
    "def modeling():\n",
    "    \n",
    "    best_score = -100\n",
    "    best_loss = np.inf \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        tk0 = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "\n",
    "        for i, (images, labels) in tk0:\n",
    "\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            y_preds = model(images)\n",
    "            loss = criterion(y_preds, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "            \n",
    "        model.eval()\n",
    "        avg_val_loss = 0.\n",
    "        preds = []\n",
    "        valid_labels = []\n",
    "        tk1 = tqdm(enumerate(valid_loader), total=len(valid_loader))\n",
    "\n",
    "        for i, (images, labels) in tk1:\n",
    "            \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                y_preds = model(images)\n",
    "            \n",
    "            preds.append(y_preds.to('cpu').numpy().argmax(1))\n",
    "            valid_labels.append(labels.to('cpu').numpy())\n",
    "\n",
    "            loss = criterion(y_preds, labels)\n",
    "            avg_val_loss += loss.item() / len(valid_loader)\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "            \n",
    "        preds = np.concatenate(preds)\n",
    "        valid_labels = np.concatenate(valid_labels)\n",
    "        \n",
    "        LOGGER.debug(f'Counter preds: {Counter(preds)}')\n",
    "        score = cohen_kappa_score(valid_labels, preds, weights='quadratic')\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        LOGGER.debug(f'  Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.debug(f'  Epoch {epoch+1} - QWK: {score}')\n",
    "        \n",
    "        if score>best_score:\n",
    "            best_score = score\n",
    "            LOGGER.debug(f'  Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save(model.state_dict(), 'SE_ResNet50_32x4d.pth') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3981/3981 [06:20<00:00, 10.45it/s]\n",
      "100%|██████████| 1327/1327 [00:35<00:00, 37.91it/s]\n",
      "2020-09-17 22:20:06,290 DEBUG Counter preds: Counter({0: 1154, 1: 638, 5: 383, 2: 303, 3: 131, 4: 45})\n",
      "2020-09-17 22:20:06,296 DEBUG   Epoch 1 - avg_train_loss: 1.5514  avg_val_loss: 1.4486  time: 416s\n",
      "2020-09-17 22:20:06,297 DEBUG   Epoch 1 - QWK: 0.5267082802222285\n",
      "2020-09-17 22:20:06,298 DEBUG   Epoch 1 - Save Best Score: 0.5267 Model\n",
      "100%|██████████| 3981/3981 [06:20<00:00, 10.46it/s]\n",
      "100%|██████████| 1327/1327 [00:33<00:00, 39.20it/s]\n",
      "2020-09-17 22:27:01,333 DEBUG Counter preds: Counter({0: 951, 1: 605, 5: 600, 2: 345, 3: 121, 4: 32})\n",
      "2020-09-17 22:27:01,338 DEBUG   Epoch 2 - avg_train_loss: 1.4489  avg_val_loss: 1.4483  time: 415s\n",
      "2020-09-17 22:27:01,339 DEBUG   Epoch 2 - QWK: 0.5525356259155748\n",
      "2020-09-17 22:27:01,339 DEBUG   Epoch 2 - Save Best Score: 0.5525 Model\n",
      "100%|██████████| 3981/3981 [06:21<00:00, 10.42it/s]\n",
      "100%|██████████| 1327/1327 [00:34<00:00, 38.96it/s]\n",
      "2020-09-17 22:33:57,849 DEBUG Counter preds: Counter({1: 1152, 0: 852, 5: 481, 2: 169})\n",
      "2020-09-17 22:33:57,853 DEBUG   Epoch 3 - avg_train_loss: 1.5773  avg_val_loss: 1.5593  time: 416s\n",
      "2020-09-17 22:33:57,854 DEBUG   Epoch 3 - QWK: 0.46433602551787\n",
      "100%|██████████| 3981/3981 [06:19<00:00, 10.48it/s]\n",
      "100%|██████████| 1327/1327 [00:33<00:00, 39.18it/s]\n",
      "2020-09-17 22:40:51,894 DEBUG Counter preds: Counter({0: 1080, 1: 587, 2: 449, 4: 397, 5: 131, 3: 10})\n",
      "2020-09-17 22:40:51,899 DEBUG   Epoch 4 - avg_train_loss: 1.5163  avg_val_loss: 1.5978  time: 414s\n",
      "2020-09-17 22:40:51,900 DEBUG   Epoch 4 - QWK: 0.48880429307558027\n",
      "100%|██████████| 3981/3981 [06:19<00:00, 10.48it/s]\n",
      "100%|██████████| 1327/1327 [00:34<00:00, 38.98it/s]\n",
      "2020-09-17 22:47:46,161 DEBUG Counter preds: Counter({0: 1451, 5: 495, 1: 475, 2: 196, 4: 22, 3: 15})\n",
      "2020-09-17 22:47:46,166 DEBUG   Epoch 5 - avg_train_loss: 1.4739  avg_val_loss: 1.5776  time: 414s\n",
      "2020-09-17 22:47:46,166 DEBUG   Epoch 5 - QWK: 0.492649698249776\n",
      "100%|██████████| 3981/3981 [06:20<00:00, 10.47it/s]\n",
      "100%|██████████| 1327/1327 [00:33<00:00, 39.16it/s]\n",
      "2020-09-17 22:54:40,621 DEBUG Counter preds: Counter({0: 1034, 1: 972, 5: 321, 3: 307, 4: 18, 2: 2})\n",
      "2020-09-17 22:54:40,625 DEBUG   Epoch 6 - avg_train_loss: 1.4452  avg_val_loss: 1.4869  time: 414s\n",
      "2020-09-17 22:54:40,626 DEBUG   Epoch 6 - QWK: 0.5157953589614879\n",
      "100%|██████████| 3981/3981 [06:19<00:00, 10.48it/s]\n",
      "100%|██████████| 1327/1327 [00:33<00:00, 39.08it/s]\n",
      "2020-09-17 23:01:34,534 DEBUG Counter preds: Counter({0: 926, 1: 869, 4: 379, 5: 288, 2: 163, 3: 29})\n",
      "2020-09-17 23:01:34,539 DEBUG   Epoch 7 - avg_train_loss: 1.4077  avg_val_loss: 1.5364  time: 414s\n",
      "2020-09-17 23:01:34,539 DEBUG   Epoch 7 - QWK: 0.5157022804670031\n",
      "100%|██████████| 3981/3981 [06:20<00:00, 10.46it/s]\n",
      "100%|██████████| 1327/1327 [00:33<00:00, 39.12it/s]\n",
      "2020-09-17 23:08:29,490 DEBUG Counter preds: Counter({0: 779, 5: 773, 1: 611, 4: 254, 2: 174, 3: 63})\n",
      "2020-09-17 23:08:29,494 DEBUG   Epoch 8 - avg_train_loss: 1.3900  avg_val_loss: 1.5163  time: 415s\n",
      "2020-09-17 23:08:29,494 DEBUG   Epoch 8 - QWK: 0.5746084466033514\n",
      "2020-09-17 23:08:29,495 DEBUG   Epoch 8 - Save Best Score: 0.5746 Model\n",
      "100%|██████████| 3981/3981 [06:20<00:00, 10.45it/s]\n",
      "100%|██████████| 1327/1327 [00:34<00:00, 38.46it/s]\n",
      "2020-09-17 23:15:25,953 DEBUG Counter preds: Counter({1: 911, 0: 839, 5: 348, 3: 246, 2: 164, 4: 146})\n",
      "2020-09-17 23:15:25,957 DEBUG   Epoch 9 - avg_train_loss: 1.3733  avg_val_loss: 1.4175  time: 416s\n",
      "2020-09-17 23:15:25,958 DEBUG   Epoch 9 - QWK: 0.5800912883963034\n",
      "2020-09-17 23:15:25,959 DEBUG   Epoch 9 - Save Best Score: 0.5801 Model\n",
      "100%|██████████| 3981/3981 [06:21<00:00, 10.44it/s]\n",
      "100%|██████████| 1327/1327 [00:33<00:00, 39.13it/s]\n",
      "2020-09-17 23:22:21,698 DEBUG Counter preds: Counter({0: 886, 5: 714, 1: 648, 2: 225, 3: 120, 4: 61})\n",
      "2020-09-17 23:22:21,703 DEBUG   Epoch 10 - avg_train_loss: 1.3593  avg_val_loss: 1.4522  time: 416s\n",
      "2020-09-17 23:22:21,704 DEBUG   Epoch 10 - QWK: 0.5813959969861575\n",
      "2020-09-17 23:22:21,704 DEBUG   Epoch 10 - Save Best Score: 0.5814 Model\n",
      "100%|██████████| 3981/3981 [06:21<00:00, 10.45it/s]\n",
      "100%|██████████| 1327/1327 [00:33<00:00, 39.49it/s]\n",
      "2020-09-17 23:29:16,886 DEBUG Counter preds: Counter({0: 941, 1: 571, 5: 420, 3: 394, 2: 261, 4: 67})\n",
      "2020-09-17 23:29:16,890 DEBUG   Epoch 11 - avg_train_loss: 1.3428  avg_val_loss: 1.4484  time: 415s\n",
      "2020-09-17 23:29:16,890 DEBUG   Epoch 11 - QWK: 0.5872433428600927\n",
      "2020-09-17 23:29:16,891 DEBUG   Epoch 11 - Save Best Score: 0.5872 Model\n",
      "100%|██████████| 3981/3981 [06:19<00:00, 10.48it/s]\n",
      "100%|██████████| 1327/1327 [00:34<00:00, 38.67it/s]\n",
      "2020-09-17 23:36:11,751 DEBUG Counter preds: Counter({0: 992, 1: 577, 5: 552, 2: 331, 3: 127, 4: 75})\n",
      "2020-09-17 23:36:11,755 DEBUG   Epoch 12 - avg_train_loss: 1.3208  avg_val_loss: 1.5391  time: 415s\n",
      "2020-09-17 23:36:11,756 DEBUG   Epoch 12 - QWK: 0.559316426914934\n",
      "100%|██████████| 3981/3981 [06:20<00:00, 10.45it/s]\n",
      "100%|██████████| 1327/1327 [00:33<00:00, 39.07it/s]\n",
      "2020-09-17 23:43:06,771 DEBUG Counter preds: Counter({1: 831, 0: 813, 5: 536, 3: 227, 4: 149, 2: 98})\n",
      "2020-09-17 23:43:06,775 DEBUG   Epoch 13 - avg_train_loss: 1.3171  avg_val_loss: 1.4472  time: 415s\n",
      "2020-09-17 23:43:06,776 DEBUG   Epoch 13 - QWK: 0.5533637802823241\n",
      "100%|██████████| 3981/3981 [06:20<00:00, 10.46it/s]\n",
      "100%|██████████| 1327/1327 [00:33<00:00, 39.22it/s]\n",
      "2020-09-17 23:50:01,419 DEBUG Counter preds: Counter({0: 1045, 1: 632, 3: 443, 5: 306, 2: 137, 4: 91})\n",
      "2020-09-17 23:50:01,423 DEBUG   Epoch 14 - avg_train_loss: 1.2969  avg_val_loss: 1.3811  time: 415s\n",
      "2020-09-17 23:50:01,424 DEBUG   Epoch 14 - QWK: 0.6045931688807948\n",
      "2020-09-17 23:50:01,425 DEBUG   Epoch 14 - Save Best Score: 0.6046 Model\n",
      "100%|██████████| 3981/3981 [06:20<00:00, 10.46it/s]\n",
      "100%|██████████| 1327/1327 [00:34<00:00, 38.68it/s]\n",
      "2020-09-17 23:56:56,833 DEBUG Counter preds: Counter({1: 905, 0: 588, 5: 383, 2: 321, 3: 317, 4: 140})\n",
      "2020-09-17 23:56:56,837 DEBUG   Epoch 15 - avg_train_loss: 1.2856  avg_val_loss: 1.4942  time: 415s\n",
      "2020-09-17 23:56:56,838 DEBUG   Epoch 15 - QWK: 0.592892931588582\n",
      "100%|██████████| 3981/3981 [06:21<00:00, 10.45it/s]\n",
      "100%|██████████| 1327/1327 [00:33<00:00, 39.22it/s]\n",
      "2020-09-18 00:03:52,045 DEBUG Counter preds: Counter({0: 1076, 1: 488, 2: 363, 4: 268, 5: 250, 3: 209})\n",
      "2020-09-18 00:03:52,050 DEBUG   Epoch 16 - avg_train_loss: 1.2675  avg_val_loss: 1.3842  time: 415s\n",
      "2020-09-18 00:03:52,050 DEBUG   Epoch 16 - QWK: 0.6051475393885768\n",
      "2020-09-18 00:03:52,051 DEBUG   Epoch 16 - Save Best Score: 0.6051 Model\n",
      "100%|██████████| 3981/3981 [06:21<00:00, 10.45it/s]\n",
      "100%|██████████| 1327/1327 [00:33<00:00, 39.52it/s]\n",
      "2020-09-18 00:10:47,179 DEBUG Counter preds: Counter({0: 958, 1: 593, 5: 497, 2: 217, 4: 210, 3: 179})\n",
      "2020-09-18 00:10:47,183 DEBUG   Epoch 17 - avg_train_loss: 1.2470  avg_val_loss: 1.4304  time: 415s\n",
      "2020-09-18 00:10:47,183 DEBUG   Epoch 17 - QWK: 0.599236896419861\n",
      "100%|██████████| 3981/3981 [06:19<00:00, 10.49it/s]\n",
      "100%|██████████| 1327/1327 [00:34<00:00, 38.43it/s]\n",
      "2020-09-18 00:17:41,392 DEBUG Counter preds: Counter({0: 908, 1: 619, 5: 559, 2: 304, 3: 158, 4: 106})\n",
      "2020-09-18 00:17:41,396 DEBUG   Epoch 18 - avg_train_loss: 1.2251  avg_val_loss: 1.5497  time: 414s\n",
      "2020-09-18 00:17:41,397 DEBUG   Epoch 18 - QWK: 0.5822590932869639\n",
      "100%|██████████| 3981/3981 [06:20<00:00, 10.46it/s]\n",
      "100%|██████████| 1327/1327 [00:34<00:00, 38.59it/s]\n",
      "2020-09-18 00:24:36,794 DEBUG Counter preds: Counter({0: 888, 1: 665, 5: 484, 3: 245, 2: 200, 4: 172})\n",
      "2020-09-18 00:24:36,799 DEBUG   Epoch 19 - avg_train_loss: 1.1997  avg_val_loss: 1.4575  time: 415s\n",
      "2020-09-18 00:24:36,800 DEBUG   Epoch 19 - QWK: 0.5934430430047397\n",
      "100%|██████████| 3981/3981 [06:21<00:00, 10.44it/s]\n",
      "100%|██████████| 1327/1327 [00:34<00:00, 38.59it/s]\n",
      "2020-09-18 00:31:32,655 DEBUG Counter preds: Counter({0: 918, 5: 577, 1: 529, 2: 327, 4: 232, 3: 71})\n",
      "2020-09-18 00:31:32,659 DEBUG   Epoch 20 - avg_train_loss: 1.1903  avg_val_loss: 1.4778  time: 416s\n",
      "2020-09-18 00:31:32,660 DEBUG   Epoch 20 - QWK: 0.5887019645497875\n"
     ]
    }
   ],
   "source": [
    "data_setup()\n",
    "build_the_network()\n",
    "optim() \n",
    "modeling()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
